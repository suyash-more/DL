{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "533e4ae8",
   "metadata": {
    "id": "533e4ae8"
   },
   "source": [
    "Name: **Ruturaj Kiran Patil**<br>\n",
    "Div: **BE09-S09**<br>\n",
    "Roll no: **43165**<br>\n",
    "Title: **Assignment 6: Object detection using Transfer Learning of CNN architectures**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce2c84b5",
   "metadata": {
    "id": "ce2c84b5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F  # All functions that don't have any parameters\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    ")  # Gives easier dataset managment and creates mini batches\n",
    "import torchvision.datasets as datasets  # Has standard datasets we can import in a nice way\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46c8e517",
   "metadata": {
    "id": "46c8e517"
   },
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "num_classes = 10\n",
    "learning_rate = 1e-3\n",
    "batch_size = 1024\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31c5720f",
   "metadata": {
    "id": "31c5720f"
   },
   "outputs": [],
   "source": [
    "# Simple Identity class that let's input pass without changes\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa50c497",
   "metadata": {
    "id": "fa50c497"
   },
   "source": [
    "### 1. Load in a pretrained model (VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3191738",
   "metadata": {
    "id": "a3191738"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruturajpatil/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ruturajpatil/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/ruturajpatil/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c6b3697",
   "metadata": {
    "id": "3c6b3697",
    "outputId": "57fd57c7-df28-483b-884e-ff1b85c3c80f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066a821d",
   "metadata": {
    "id": "066a821d"
   },
   "source": [
    "### 2. Freezing parameters in model's lower layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65233d19",
   "metadata": {
    "id": "65233d19"
   },
   "outputs": [],
   "source": [
    "# If you want to do finetuning then set requires_grad = False\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61c4fdeb",
   "metadata": {
    "id": "61c4fdeb"
   },
   "outputs": [],
   "source": [
    "## Freezing the average pool layer of the model and add a custom classifier\n",
    "model.avgpool = Identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df03867",
   "metadata": {
    "id": "5df03867"
   },
   "source": [
    "### 3. Add custom classifier with several layers of trainable parameters to mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c18b8006",
   "metadata": {
    "id": "c18b8006",
    "outputId": "33fb82c8-ca9e-4122-9756-b87dc25ab252"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): Identity()\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(512, 100), nn.ReLU(), \n",
    "    nn.Linear(100, num_classes)\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991845db",
   "metadata": {
    "id": "991845db"
   },
   "source": [
    "### 4. Train classifier layers on training data available for task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "486c32c1",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "9dead8c68ad1491babfc1b411a47da1d"
     ]
    },
    "id": "486c32c1",
    "outputId": "8395b0e1-40db-481c-d718-18cb00290892"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to dataset/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/cifar-10-python.tar.gz to dataset/\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True\n",
    ")\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06fa938d",
   "metadata": {
    "id": "06fa938d"
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78e6681c",
   "metadata": {
    "id": "78e6681c",
    "outputId": "18703e04-9788-45d0-aa67-33903003ce01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0 is 0.74452\n",
      "Cost at epoch 1 is 0.73771\n",
      "Cost at epoch 2 is 0.73279\n",
      "Cost at epoch 3 is 0.72999\n",
      "Cost at epoch 4 is 0.72231\n",
      "Cost at epoch 5 is 0.72027\n",
      "Cost at epoch 6 is 0.71365\n",
      "Cost at epoch 7 is 0.70845\n",
      "Cost at epoch 8 is 0.70504\n",
      "Cost at epoch 9 is 0.70078\n",
      "Cost at epoch 10 is 0.69512\n",
      "Cost at epoch 11 is 0.68973\n",
      "Cost at epoch 12 is 0.68553\n",
      "Cost at epoch 13 is 0.68230\n",
      "Cost at epoch 14 is 0.67616\n",
      "Cost at epoch 15 is 0.67199\n",
      "Cost at epoch 16 is 0.66721\n",
      "Cost at epoch 17 is 0.66456\n",
      "Cost at epoch 18 is 0.65900\n",
      "Cost at epoch 19 is 0.65321\n",
      "Cost at epoch 20 is 0.64894\n",
      "Cost at epoch 21 is 0.64827\n",
      "Cost at epoch 22 is 0.64559\n",
      "Cost at epoch 23 is 0.63880\n",
      "Cost at epoch 24 is 0.63768\n",
      "Cost at epoch 25 is 0.63080\n",
      "Cost at epoch 26 is 0.62703\n",
      "Cost at epoch 27 is 0.62392\n",
      "Cost at epoch 28 is 0.62043\n",
      "Cost at epoch 29 is 0.61719\n",
      "Cost at epoch 30 is 0.61401\n",
      "Cost at epoch 31 is 0.61015\n",
      "Cost at epoch 32 is 0.60704\n",
      "Cost at epoch 33 is 0.60256\n",
      "Cost at epoch 34 is 0.60116\n",
      "Cost at epoch 35 is 0.59558\n",
      "Cost at epoch 36 is 0.59220\n",
      "Cost at epoch 37 is 0.59090\n",
      "Cost at epoch 38 is 0.58601\n",
      "Cost at epoch 39 is 0.58393\n",
      "Cost at epoch 40 is 0.57806\n",
      "Cost at epoch 41 is 0.57598\n",
      "Cost at epoch 42 is 0.57466\n",
      "Cost at epoch 43 is 0.57111\n",
      "Cost at epoch 44 is 0.56705\n",
      "Cost at epoch 45 is 0.56460\n",
      "Cost at epoch 46 is 0.56366\n",
      "Cost at epoch 47 is 0.55820\n",
      "Cost at epoch 48 is 0.55514\n",
      "Cost at epoch 49 is 0.55108\n"
     ]
    }
   ],
   "source": [
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Cost at epoch {epoch} is {sum(losses)/len(losses):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3196dab",
   "metadata": {
    "id": "b3196dab"
   },
   "source": [
    "### 5. Checking accuracy and fine tuning if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c35db4c",
   "metadata": {
    "id": "0c35db4c"
   },
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"Checking accuracy on training data\")\n",
    "    else:\n",
    "        print(\"Checking accuracy on test data\")\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "        print(\n",
    "            f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}\"\n",
    "        )\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ab1886",
   "metadata": {
    "id": "04ab1886",
    "outputId": "7790c4bf-d8ab-42e8-becd-3a03883974a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on training data\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(train_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f88ee",
   "metadata": {
    "id": "0d6f88ee"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "abb966b7ea35b20666f3a996666172c7b349752d6f16e30915fecc2a1a4bf9ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
